"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–µ—Ü–µ–ø—Ç–æ–≤
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import tensorflow as tf
from pathlib import Path
import json
import argparse
import logging

from src.models.terrazite_model import TerraziteRecipeModel
from src.data.loader import DataLoader, RecipeLoader
from src.data.processor import DataProcessor
from src.utils.logger import setup_logger

logger = setup_logger()


def prepare_training_data(data_dir: str):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {data_dir}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ä–µ—Ü–µ–ø—Ç–æ–≤
    images, recipes, aggregate_types = DataLoader.load_dataset(data_dir)
    
    if not images:
        logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –°–æ–∑–¥–∞—é —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...")
        return create_synthetic_data()
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X = np.array(images)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    processor = DataProcessor()
    features, valid_recipes = processor.prepare_recipe_features(recipes)
    targets = processor.prepare_targets(valid_recipes)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    dataset = processor.split_dataset(features, targets, test_size=0.2, val_size=0.1)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∫—Ç–∏–≤–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞)
    # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ –∑–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å–≤—è–∑—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Ä–µ—Ü–µ–ø—Ç–∞–º–∏
    X_train = X[:len(dataset['X_train'])] if len(X) >= len(dataset['X_train']) else np.random.rand(len(dataset['X_train']), 224, 224, 3)
    X_val = X[len(dataset['X_train']):len(dataset['X_train'])+len(dataset['X_val'])] if len(X) >= len(dataset['X_train'])+len(dataset['X_val']) else np.random.rand(len(dataset['X_val']), 224, 224, 3)
    X_test = X[-len(dataset['X_test']):] if len(X) >= len(dataset['X_test']) else np.random.rand(len(dataset['X_test']), 224, 224, 3)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è –º–æ–¥–µ–ª–∏
    y_train = {
        'regression_output': dataset['y_reg_train'],
        'classification_output': tf.keras.utils.to_categorical(dataset['y_cls_train'], len(targets['class_names']))
    }
    
    y_val = {
        'regression_output': dataset['y_reg_val'],
        'classification_output': tf.keras.utils.to_categorical(dataset['y_cls_val'], len(targets['class_names']))
    }
    
    return (X_train, y_train), (X_val, y_val), (X_test, dataset['y_reg_test'], dataset['y_cls_test']), targets['class_names']


def create_synthetic_data():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    n_samples = 100
    X_train = np.random.rand(n_samples, 224, 224, 3).astype('float32')
    X_val = np.random.rand(20, 224, 224, 3).astype('float32')
    X_test = np.random.rand(30, 224, 224, 3).astype('float32')
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –º–µ—Ç–∫–∏
    y_reg_train = np.random.rand(n_samples, 15).astype('float32')
    y_reg_val = np.random.rand(20, 15).astype('float32')
    y_reg_test = np.random.rand(30, 15).astype('float32')
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —á—Ç–æ–±—ã —Å—É–º–º–∞ –±—ã–ª–∞ 1
    y_reg_train = y_reg_train / y_reg_train.sum(axis=1, keepdims=True)
    y_reg_val = y_reg_val / y_reg_val.sum(axis=1, keepdims=True)
    y_reg_test = y_reg_test / y_reg_test.sum(axis=1, keepdims=True)
    
    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    n_classes = 5
    y_cls_train = np.random.randint(0, n_classes, n_samples)
    y_cls_val = np.random.randint(0, n_classes, 20)
    y_cls_test = np.random.randint(0, n_classes, 30)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ one-hot
    y_cls_train_onehot = tf.keras.utils.to_categorical(y_cls_train, n_classes)
    y_cls_val_onehot = tf.keras.utils.to_categorical(y_cls_val, n_classes)
    
    y_train = {
        'regression_output': y_reg_train,
        'classification_output': y_cls_train_onehot
    }
    
    y_val = {
        'regression_output': y_reg_val,
        'classification_output': y_cls_val_onehot
    }
    
    class_names = ['–º—Ä–∞–º–æ—Ä', '–∫–≤–∞—Ä—Ü', '–≥—Ä–∞–Ω–∏—Ç', '—Å–ª—é–¥–∞', '–∏–∑–≤–µ—Å—Ç–Ω—è–∫']
    
    return (X_train, y_train), (X_val, y_val), (X_test, y_reg_test, y_cls_test), class_names


def train_model(args):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    logger.info("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    (X_train, y_train), (X_val, y_val), (X_test, y_reg_test, y_cls_test), class_names = prepare_training_data(args.data_dir)
    
    logger.info(f"–î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")
    logger.info(f"  –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    logger.info(f"  –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_val)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    logger.info(f"  –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    logger.info(f"  –ö–ª–∞—Å—Å—ã: {class_names}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = TerraziteRecipeModel(
        num_regression_outputs=15,
        num_classes=len(class_names),
        learning_rate=args.learning_rate,
        dropout_rate=args.dropout_rate
    )
    
    model.build_model()
    
    # –û–±—É—á–µ–Ω–∏–µ
    logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {args.epochs} —ç–ø–æ—Ö...")
    history = model.train(
        train_data=(X_train, y_train),
        val_data=(X_val, y_val),
        epochs=args.epochs,
        batch_size=args.batch_size
    )
    
    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    logger.info("–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    test_data = (X_test, {
        'regression_output': y_reg_test,
        'classification_output': tf.keras.utils.to_categorical(y_cls_test, len(class_names))
    })
    
    metrics = model.evaluate(test_data)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model.save_model(args.model_path)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
    history_path = Path(args.model_path).parent / "training_history.json"
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    metrics_path = Path(args.model_path).parent / "training_metrics.json"
    with open(metrics_path, 'w') as f:
        json.dump(metrics, f, indent=2)
    
    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {args.model_path}")
    logger.info(f"üìä –ú–µ—Ç—Ä–∏–∫–∏: {metrics}")
    
    return model, history, metrics


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞"""
    parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Terrazite AI")
    parser.add_argument('--data-dir', type=str, default='data/processed',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏')
    parser.add_argument('--model-path', type=str, default='models/terrazite_model.h5',
                       help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏')
    parser.add_argument('--epochs', type=int, default=50,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--batch-size', type=int, default=32,
                       help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞')
    parser.add_argument('--learning-rate', type=float, default=0.001,
                       help='–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--dropout-rate', type=float, default=0.3,
                       help='Dropout rate')
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    Path(args.model_path).parent.mkdir(parents=True, exist_ok=True)
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    train_model(args)


if __name__ == "__main__":
    main()
