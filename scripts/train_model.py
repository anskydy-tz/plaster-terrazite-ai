"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–µ—Ü–µ–ø—Ç–æ–≤ —Ç–µ—Ä—Ä–∞–∑–∏—Ç–æ–≤–æ–π —à—Ç—É–∫–∞—Ç—É—Ä–∫–∏
–û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import tensorflow as tf
from pathlib import Path
import json
import argparse
import logging
import pandas as pd

from src.models.terrazite_model import TerraziteRecipeModel
from src.data.loader import ManifestDataLoader, RecipeLoader
from src.data.processor import DataProcessor
from src.utils.logger import setup_logger

logger = setup_logger()


def prepare_training_data_from_manifest(data_dir: str, target_size=(224, 224)):
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–≤
    
    Args:
        data_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏
        target_size: –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    
    Returns:
        (X_train, y_train), (X_val, y_val), (X_test, y_test), component_names
    """
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–≤ –≤ {data_dir}")
    
    try:
        # –°–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫ –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–≤
        loader = ManifestDataLoader(data_dir)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        datasets = loader.prepare_training_data(
            train_manifest="train",
            val_manifest="val",
            test_manifest="test",
            recipes_json=os.path.join(data_dir, "recipes.json"),
            target_size=target_size
        )
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        X_train = datasets['train']['images']
        y_train_reg = datasets['train']['labels']
        
        X_val = datasets['val']['images']
        y_val_reg = datasets['val']['labels']
        
        X_test = datasets['test']['images']
        y_test_reg = datasets['test']['labels']
        
        # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–∏–ø–æ–≤ —Ä–µ—Ü–µ–ø—Ç–æ–≤
        # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø—ã —Ä–µ—Ü–µ–ø—Ç–æ–≤ –∏–∑ –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–≤
        train_manifest = datasets['train']['manifest']
        val_manifest = datasets['val']['manifest']
        test_manifest = datasets['test']['manifest']
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –≤ —á–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç–∫–∏
        all_types = pd.concat([
            train_manifest['recipe_type'],
            val_manifest['recipe_type'],
            test_manifest['recipe_type']
        ]).unique()
        
        type_to_idx = {t: i for i, t in enumerate(sorted(all_types))}
        idx_to_type = {i: t for t, i in type_to_idx.items()}
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–∏–ø—ã —Ä–µ—Ü–µ–ø—Ç–æ–≤ –≤ one-hot –≤–µ–∫—Ç–æ—Ä—ã
        def create_type_labels(manifest_df):
            labels = []
            for _, row in manifest_df.iterrows():
                recipe_id = str(row['recipe_id'])
                # –ù–∞—Ö–æ–¥–∏–º —Ä–µ—Ü–µ–ø—Ç –≤ recipes.json –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–∏–ø–∞
                recipe_type = row['recipe_type']
                labels.append(type_to_idx.get(recipe_type, 0))
            return np.array(labels)
        
        y_train_cls = create_type_labels(train_manifest)
        y_val_cls = create_type_labels(val_manifest)
        y_test_cls = create_type_labels(test_manifest)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è –º–æ–¥–µ–ª–∏
        y_train = {
            'regression_output': y_train_reg,
            'classification_output': tf.keras.utils.to_categorical(
                y_train_cls, len(type_to_idx)
            )
        }
        
        y_val = {
            'regression_output': y_val_reg,
            'classification_output': tf.keras.utils.to_categorical(
                y_val_cls, len(type_to_idx)
            )
        }
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        recipes_json_path = os.path.join(data_dir, "recipes.json")
        component_names = loader.get_component_names_from_json(recipes_json_path)
        
        logger.info(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")
        logger.info(f"  –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        logger.info(f"  –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_val)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        logger.info(f"  –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        logger.info(f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {len(component_names)}")
        logger.info(f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∏–ø–æ–≤ —Ä–µ—Ü–µ–ø—Ç–æ–≤: {len(type_to_idx)}")
        logger.info(f"  –¢–∏–ø—ã —Ä–µ—Ü–µ–ø—Ç–æ–≤: {list(type_to_idx.keys())}")
        
        return (X_train, y_train), (X_val, y_val), (X_test, y_test_reg, y_test_cls), component_names, idx_to_type
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        logger.warning("–°–æ–∑–¥–∞—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è...")
        return create_synthetic_data()


def get_component_names_from_json(json_path):
    """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–º–µ–Ω –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏–∑ JSON"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            recipes = json.load(f)
        
        all_components = set()
        for recipe in recipes:
            all_components.update(recipe.get('components', {}).keys())
        
        return sorted(list(all_components))
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {e}")
        return []


# –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–æ–¥ –≤ ManifestDataLoader —á–µ—Ä–µ–∑ monkey patch –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
ManifestDataLoader.get_component_names_from_json = staticmethod(get_component_names_from_json)


def create_synthetic_data():
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (—Ä–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)
    """
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    n_samples = 100
    X_train = np.random.rand(n_samples, 224, 224, 3).astype('float32')
    X_val = np.random.rand(20, 224, 224, 3).astype('float32')
    X_test = np.random.rand(30, 224, 224, 3).astype('float32')
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (–∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã)
    n_components = 15
    y_reg_train = np.random.rand(n_samples, n_components).astype('float32')
    y_reg_val = np.random.rand(20, n_components).astype('float32')
    y_reg_test = np.random.rand(30, n_components).astype('float32')
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —á—Ç–æ–±—ã —Å—É–º–º–∞ –±—ã–ª–∞ 1
    y_reg_train = y_reg_train / y_reg_train.sum(axis=1, keepdims=True)
    y_reg_val = y_reg_val / y_reg_val.sum(axis=1, keepdims=True)
    y_reg_test = y_reg_test / y_reg_test.sum(axis=1, keepdims=True)
    
    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    n_classes = 3  # –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è, —Ñ–∞—Å–∞–¥–Ω–∞—è, –¥–µ–∫–æ—Ä–∞—Ç–∏–≤–Ω–∞—è
    y_cls_train = np.random.randint(0, n_classes, n_samples)
    y_cls_val = np.random.randint(0, n_classes, 20)
    y_cls_test = np.random.randint(0, n_classes, 30)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ one-hot
    y_cls_train_onehot = tf.keras.utils.to_categorical(y_cls_train, n_classes)
    y_cls_val_onehot = tf.keras.utils.to_categorical(y_cls_val, n_classes)
    
    y_train = {
        'regression_output': y_reg_train,
        'classification_output': y_cls_train_onehot
    }
    
    y_val = {
        'regression_output': y_reg_val,
        'classification_output': y_cls_val_onehot
    }
    
    component_names = [
        '–º—Ä–∞–º–æ—Ä', '–∫–≤–∞—Ä—Ü', '–≥—Ä–∞–Ω–∏—Ç', '—Å–ª—é–¥–∞', '–∏–∑–≤–µ—Å—Ç–Ω—è–∫', 
        '—Ü–µ–º–µ–Ω—Ç', '–ø–µ—Å–æ–∫', '–≤–æ–¥–∞', '–ø–∏–≥–º–µ–Ω—Ç_–∫—Ä–∞—Å–Ω—ã–π', 
        '–ø–∏–≥–º–µ–Ω—Ç_—Å–∏–Ω–∏–π', '–ø–∏–≥–º–µ–Ω—Ç_–∂–µ–ª—Ç—ã–π', '–ø–ª–∞—Å—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä', 
        '–≤–æ–ª–æ–∫–Ω–æ', '–¥–æ–±–∞–≤–∫–∞_1', '–¥–æ–±–∞–≤–∫–∞_2'
    ]
    
    idx_to_type = {0: '–≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è', 1: '—Ñ–∞—Å–∞–¥–Ω–∞—è', 2: '–¥–µ–∫–æ—Ä–∞—Ç–∏–≤–Ω–∞—è'}
    
    return (X_train, y_train), (X_val, y_val), (X_test, y_reg_test, y_cls_test), component_names, idx_to_type


def train_model(args):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    logger.info("="*60)
    logger.info("üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò TERRAZITE AI")
    logger.info("="*60)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    logger.info("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    (X_train, y_train), (X_val, y_val), (X_test, y_test_reg, y_test_cls), component_names, idx_to_type = prepare_training_data_from_manifest(
        args.data_dir, target_size=(args.image_size, args.image_size)
    )
    
    logger.info(f"üìà –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")
    logger.info(f"  –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    logger.info(f"  –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_val)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    logger.info(f"  –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    logger.info(f"  –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {len(component_names)}")
    logger.info(f"  –¢–∏–ø–æ–≤ —Ä–µ—Ü–µ–ø—Ç–æ–≤: {len(idx_to_type)}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    logger.info("üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = TerraziteRecipeModel(
        num_regression_outputs=len(component_names),
        num_classes=len(idx_to_type),
        learning_rate=args.learning_rate,
        dropout_rate=args.dropout_rate,
        image_size=args.image_size
    )
    
    model.build_model()
    
    # –û–±—É—á–µ–Ω–∏–µ
    logger.info(f"üéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {args.epochs} —ç–ø–æ—Ö...")
    logger.info(f"  –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {args.batch_size}")
    logger.info(f"  –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è: {args.learning_rate}")
    
    history = model.train(
        train_data=(X_train, y_train),
        val_data=(X_val, y_val),
        epochs=args.epochs,
        batch_size=args.batch_size,
        use_early_stopping=args.early_stopping,
        patience=args.patience
    )
    
    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    logger.info("üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    test_data = (X_test, {
        'regression_output': y_test_reg,
        'classification_output': tf.keras.utils.to_categorical(y_test_cls, len(idx_to_type))
    })
    
    metrics = model.evaluate(test_data)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model_save_path = args.model_path
    model.save_model(model_save_path)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
    history_path = Path(model_save_path).parent / "training_history.json"
    with open(history_path, 'w') as f:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy —Ç–∏–ø—ã –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ Python —Ç–∏–ø—ã
        history_serializable = {}
        for key, values in history.history.items():
            history_serializable[key] = [float(v) for v in values]
        json.dump(history_serializable, f, indent=2)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    metrics_path = Path(model_save_path).parent / "training_metrics.json"
    metrics_serializable = {k: float(v) for k, v in metrics.items()}
    with open(metrics_path, 'w') as f:
        json.dump(metrics_serializable, f, indent=2)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö –∏ —Ç–∏–ø–∞—Ö
    info_path = Path(model_save_path).parent / "model_info.json"
    model_info = {
        'component_names': component_names,
        'recipe_types': idx_to_type,
        'num_components': len(component_names),
        'num_types': len(idx_to_type),
        'image_size': args.image_size,
        'training_date': pd.Timestamp.now().isoformat(),
        'dataset_info': {
            'train_samples': len(X_train),
            'val_samples': len(X_val),
            'test_samples': len(X_test)
        }
    }
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(model_info, f, indent=2, ensure_ascii=False)
    
    logger.info(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_save_path}")
    logger.info(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {metrics}")
    logger.info(f"üìÑ –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è: {history_path}")
    logger.info(f"üìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏: {info_path}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if args.create_plots:
        try:
            plot_dir = Path(args.model_path).parent / "plots"
            plot_dir.mkdir(exist_ok=True)
            
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –Ω–µ —Ç—Ä–µ–±–æ–≤–∞—Ç—å matplotlib –¥–ª—è –±–∞–∑–æ–≤–æ–π —Ä–∞–±–æ—Ç—ã
            import matplotlib.pyplot as plt
            
            # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
            plt.figure(figsize=(12, 4))
            
            plt.subplot(1, 2, 1)
            plt.plot(history.history['loss'], label='–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞')
            plt.plot(history.history['val_loss'], label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞')
            plt.title('–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å')
            plt.xlabel('–≠–ø–æ—Ö–∞')
            plt.ylabel('–ü–æ—Ç–µ—Ä–∏')
            plt.legend()
            plt.grid(True)
            
            # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
            plt.subplot(1, 2, 2)
            if 'classification_output_accuracy' in history.history:
                plt.plot(history.history['classification_output_accuracy'], label='–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞')
                plt.plot(history.history['val_classification_output_accuracy'], label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞')
                plt.title('–¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏')
                plt.xlabel('–≠–ø–æ—Ö–∞')
                plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
                plt.legend()
                plt.grid(True)
            
            plt.tight_layout()
            plot_path = plot_dir / "training_history.png"
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            logger.info(f"üìà –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {plot_path}")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏: {e}")
    
    logger.info("="*60)
    logger.info("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ó–ê–í–ï–†–®–ï–ù–û")
    logger.info("="*60)
    
    return model, history, metrics


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞"""
    parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Terrazite AI")
    parser.add_argument('--data-dir', type=str, default='data/processed',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: data/processed)')
    parser.add_argument('--model-path', type=str, default='models/terrazite_model.h5',
                       help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: models/terrazite_model.h5)')
    parser.add_argument('--epochs', type=int, default=50,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 50)')
    parser.add_argument('--batch-size', type=int, default=32,
                       help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 32)')
    parser.add_argument('--learning-rate', type=float, default=0.001,
                       help='–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.001)')
    parser.add_argument('--dropout-rate', type=float, default=0.3,
                       help='Dropout rate (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.3)')
    parser.add_argument('--image-size', type=int, default=224,
                       help='–†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 224)')
    parser.add_argument('--early-stopping', action='store_true',
                       help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–Ω–Ω—é—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É')
    parser.add_argument('--patience', type=int, default=10,
                       help='Patience –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)')
    parser.add_argument('--create-plots', action='store_true',
                       help='–°–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è')
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    model_dir = Path(args.model_path).parent
    model_dir.mkdir(parents=True, exist_ok=True)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö
    data_dir = Path(args.data_dir)
    required_files = [
        data_dir / "data_manifest_train.csv",
        data_dir / "data_manifest_val.csv", 
        data_dir / "data_manifest_test.csv",
        data_dir / "recipes.json"
    ]
    
    missing_files = [f for f in required_files if not f.exists()]
    if missing_files:
        logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã: {missing_files}")
        logger.info("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–æ–∑–¥–∞–Ω–∏–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–≤: python scripts/create_data_manifest.py")
        if len(missing_files) == len(required_files):
            logger.info("–ò–ª–∏ —Å–æ–∑–¥–∞–π—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: python create_test_excel.py")
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    try:
        model, history, metrics = train_model(args)
        
        print("\n" + "="*60)
        print("üéâ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
        print("="*60)
        print(f"\nüìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {args.model_path}")
        print(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
        for key, value in metrics.items():
            print(f"  {key}: {value:.4f}")
        
        print(f"\nüìà –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        print(f"  1. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª—å: python scripts/test_model.py --model-path {args.model_path}")
        print(f"  2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ API —Å–µ—Ä–≤–µ—Ä: uvicorn src.api.main:app --reload")
        print(f"  3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å: streamlit run streamlit_app.py")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)
