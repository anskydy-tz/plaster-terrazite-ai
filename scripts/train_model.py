#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ Terrazite AI.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã –∏–∑ data/processed/ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–æ–µ–∫—Ç–∞.
"""
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π
sys.path.append(str(Path(__file__).parent.parent))

import torch
import json
import argparse
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np
from typing import Dict, Any, Optional

from src.models.trainer import ModelTrainer
from src.utils.config import config, setup_config
from src.utils.logger import setup_logger

logger = setup_logger(__name__)


def validate_data_manifests() -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö.
    
    Returns:
        True –µ—Å–ª–∏ –≤—Å–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç
    """
    required_files = [
        "data/processed/data_manifest_train.csv",
        "data/processed/data_manifest_val.csv", 
        "data/processed/data_manifest_test.csv"
    ]
    
    all_exist = True
    for file_path in required_files:
        if not Path(file_path).exists():
            logger.error(f"–ú–∞–Ω–∏—Ñ–µ—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            all_exist = False
    
    if not all_exist:
        logger.info("\nüí° –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö:")
        logger.info("   python scripts/create_data_manifest.py")
        logger.info("   python scripts/prepare_image_dataset.py")
    
    return all_exist


def train_model(args: argparse.Namespace) -> tuple:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.
    
    Args:
        args: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (trainer, history, metrics)
    """
    logger.info("=" * 60)
    logger.info("–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò TERRAZITE AI")
    logger.info("=" * 60)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if args.config:
        setup_config(args.config)
        logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {args.config}")
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –∞—Ä–≥—É–º–µ–Ω—Ç—ã > –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è > –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    trainer_config = {
        'batch_size': args.batch_size or config.model.batch_size,
        'learning_rate': args.learning_rate or config.model.learning_rate,
        'epochs': args.epochs or config.model.epochs,
        'weight_decay': args.weight_decay or config.model.weight_decay,
        'device': args.device
    }
    
    logger.info("\nüìã –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø:")
    logger.info(f"  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {trainer_config['device']}")
    logger.info(f"  Batch size: {trainer_config['batch_size']}")
    logger.info(f"  Learning rate: {trainer_config['learning_rate']}")
    logger.info(f"  –≠–ø–æ—Ö–∏: {trainer_config['epochs']}")
    logger.info(f"  Weight decay: {trainer_config['weight_decay']}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–≤
    if not validate_data_manifests():
        raise FileNotFoundError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã –¥–∞–Ω–Ω—ã—Ö")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
    logger.info("\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞...")
    trainer = ModelTrainer(trainer_config)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–≤
    logger.info("\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    train_loader, val_loader, test_loader = trainer.prepare_dataloaders(
        batch_size=trainer_config['batch_size'],
        train_manifest='data/processed/data_manifest_train.csv',
        val_manifest='data/processed/data_manifest_val.csv',
        test_manifest='data/processed/data_manifest_test.csv'
    )
    
    logger.info(f"  –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_loader.dataset)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    logger.info(f"  –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(val_loader.dataset)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    logger.info(f"  –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(test_loader.dataset)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    logger.info("\nüèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = trainer.create_model()
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
    model_info = model.get_model_info() if hasattr(model, 'get_model_info') else {}
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    logger.info(f"  –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
    logger.info(f"  –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")
    if 'num_categories' in model_info:
        logger.info(f"  –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {model_info['num_categories']}")
    if 'num_components' in model_info:
        logger.info(f"  –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {model_info['num_components']}")
    
    # –û–±—É—á–µ–Ω–∏–µ
    logger.info("\nüöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø...")
    history = trainer.train(
        train_loader,
        val_loader,
        epochs=trainer_config['epochs'],
        save_path=args.save_path
    )
    
    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    logger.info("\nüìà –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò...")
    metrics = trainer.evaluate(test_loader)
    
    # –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫
    logger.info("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
    for metric_name, metric_value in metrics.items():
        if isinstance(metric_value, (int, float)):
            logger.info(f"  {metric_name}: {metric_value:.4f}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    save_training_results(trainer, history, metrics, args)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    if args.plot:
        plot_training_results(history, args.output_dir, metrics)
    
    logger.info("\n" + "=" * 60)
    logger.info("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
    logger.info("=" * 60)
    
    return trainer, history, metrics


def save_training_results(trainer: ModelTrainer,
                         history: Dict[str, list],
                         metrics: Dict[str, float],
                         args: argparse.Namespace) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è.
    
    Args:
        trainer: –û–±—É—á–µ–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä
        history: –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        metrics: –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        args: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    """
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è –º–æ–¥–µ–ª–∏
    if args.save_path:
        model_path = Path(args.save_path)
    else:
        # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –∏–º—è —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        accuracy = metrics.get('test_accuracy', 0)
        loss = metrics.get('test_loss', 0)
        model_name = f"terrazite_model_acc{accuracy:.3f}_loss{loss:.3f}_{timestamp}.pth"
        model_path = output_dir / model_name
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    trainer.save_model(str(model_path))
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = {
        'timestamp': timestamp,
        'command_args': vars(args),
        'model_config': trainer.model.get_model_info() if hasattr(trainer.model, 'get_model_info') else {},
        'training_history': {
            'loss': history.get('loss', []),
            'val_loss': history.get('val_loss', []),
            'category_accuracy': history.get('category_accuracy', []),
            'val_category_accuracy': history.get('val_category_accuracy', []),
            'lr': history.get('lr', []),
            'epochs_completed': len(history.get('loss', []))
        },
        'test_metrics': metrics,
        'model_path': str(model_path)
    }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON
    results_path = output_dir / f"training_results_{timestamp}.json"
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–∏–π README —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    readme_path = output_dir / "README.md"
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(f"# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è {timestamp}\n\n")
        f.write(f"## –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ\n")
        for metric, value in metrics.items():
            if isinstance(value, (int, float)):
                f.write(f"- **{metric}**: {value:.4f}\n")
        f.write(f"\n## –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è\n")
        for key, value in vars(args).items():
            f.write(f"- {key}: {value}\n")
        f.write(f"\n## –§–∞–π–ª—ã\n")
        f.write(f"- –ú–æ–¥–µ–ª—å: `{model_path.name}`\n")
        f.write(f"- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: `{results_path.name}`\n")
    
    logger.info(f"\nüíæ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–û–•–†–ê–ù–ï–ù–´:")
    logger.info(f"  ‚Ä¢ –ú–æ–¥–µ–ª—å: {model_path}")
    logger.info(f"  ‚Ä¢ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {results_path}")
    logger.info(f"  ‚Ä¢ README: {readme_path}")


def plot_training_results(history: Dict[str, list],
                         output_dir: str,
                         metrics: Optional[Dict[str, float]] = None) -> None:
    """
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è.
    
    Args:
        history: –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        metrics: –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É —Å —Ç—Ä–µ–º—è –≥—Ä–∞—Ñ–∏–∫–∞–º–∏
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle(f'–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è Terrazite AI - {timestamp}', fontsize=16, fontweight='bold')
    
    # 1. –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å (Loss)
    epochs = range(1, len(history.get('loss', [])) + 1)
    
    if 'loss' in history and history['loss']:
        axes[0, 0].plot(epochs, history['loss'], 'b-', label='Train Loss', linewidth=2)
    if 'val_loss' in history and history['val_loss']:
        axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)
    
    axes[0, 0].set_title('–î–∏–Ω–∞–º–∏–∫–∞ –ø–æ—Ç–µ—Ä—å (Loss)', fontsize=14)
    axes[0, 0].set_xlabel('–≠–ø–æ—Ö–∞')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫
    if metrics and 'test_loss' in metrics:
        axes[0, 0].axhline(y=metrics['test_loss'], color='g', linestyle='--', alpha=0.5, 
                           label=f"Test Loss: {metrics['test_loss']:.4f}")
        axes[0, 0].legend()
    
    # 2. –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ (Accuracy)
    if 'category_accuracy' in history and history['category_accuracy']:
        axes[0, 1].plot(epochs, history['category_accuracy'], 'b-', label='Train Accuracy', linewidth=2)
    if 'val_category_accuracy' in history and history['val_category_accuracy']:
        axes[0, 1].plot(epochs, history['val_category_accuracy'], 'r-', label='Val Accuracy', linewidth=2)
    
    axes[0, 1].set_title('–¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏', fontsize=14)
    axes[0, 1].set_xlabel('–≠–ø–æ—Ö–∞')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].set_ylim([0, 1])
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫
    if metrics and 'test_accuracy' in metrics:
        axes[0, 1].axhline(y=metrics['test_accuracy'], color='g', linestyle='--', alpha=0.5,
                           label=f"Test Accuracy: {metrics['test_accuracy']:.4f}")
        axes[0, 1].legend()
    
    # 3. –ì—Ä–∞—Ñ–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è (Learning Rate)
    if 'lr' in history and history['lr']:
        axes[1, 0].plot(epochs, history['lr'], 'g-', linewidth=2)
        axes[1, 0].set_title('–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (Learning Rate)', fontsize=14)
        axes[1, 0].set_xlabel('–≠–ø–æ—Ö–∞')
        axes[1, 0].set_ylabel('Learning Rate')
        axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True, alpha=0.3)
    else:
        axes[1, 0].text(0.5, 0.5, 'Learning Rate Schedule\nNot Available',
                       horizontalalignment='center', verticalalignment='center',
                       transform=axes[1, 0].transAxes, fontsize=12)
        axes[1, 0].set_title('–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è', fontsize=14)
    
    # 4. –°–≤–æ–¥–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    axes[1, 1].axis('off')
    info_text = "üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê\n\n"
    
    if metrics:
        for key, value in metrics.items():
            if isinstance(value, (int, float)):
                if 'accuracy' in key:
                    info_text += f"‚úÖ {key}: {value:.2%}\n"
                elif 'loss' in key:
                    info_text += f"üìâ {key}: {value:.4f}\n"
                else:
                    info_text += f"‚Ä¢ {key}: {value:.4f}\n"
    
    info_text += f"\n‚è±Ô∏è –≠–ø–æ—Ö: {len(history.get('loss', []))}"
    
    axes[1, 1].text(0.1, 0.9, info_text,
                   transform=axes[1, 1].transAxes,
                   fontsize=12,
                   verticalalignment='top',
                   family='monospace',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
    
    plt.tight_layout()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
    plot_path = output_dir / f"training_plot_{timestamp}.png"
    plt.savefig(plot_path, dpi=150, bbox_inches='tight', facecolor='white')
    plt.close()
    
    logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {plot_path}")


def test_predictions(trainer: ModelTrainer,
                    test_loader: torch.utils.data.DataLoader,
                    num_samples: int = 5) -> list:
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö.
    
    Args:
        trainer: –û–±—É—á–µ–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä
        test_loader: –ó–∞–≥—Ä—É–∑—á–∏–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        num_samples: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –ø–æ–∫–∞–∑–∞
        
    Returns:
        –°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏
    """
    logger.info(f"\nüîç –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô ({num_samples} –ø—Ä–∏–º–µ—Ä–æ–≤)...")
    
    trainer.model.eval()
    examples = []
    
    with torch.no_grad():
        for i, batch in enumerate(test_loader):
            if i >= num_samples:
                break
            
            images = batch['image'].to(trainer.device)
            categories = batch['category'].to(trainer.device)
            recipe_names = batch.get('name', [f"–ü—Ä–∏–º–µ—Ä_{i}"] * len(images))
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            outputs = trainer.model(images)
            category_probs = torch.softmax(outputs['category_logits'], dim=1)
            predicted = torch.argmax(category_probs, dim=1)
            
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
            idx_to_category = {v: k for k, v in trainer.dataset.category_to_idx.items()}
            
            for j in range(len(images)):
                true_cat = categories[j].item()
                pred_cat = predicted[j].item()
                confidence = category_probs[j, pred_cat].item()
                
                example = {
                    'recipe_name': recipe_names[j] if isinstance(recipe_names, list) else recipe_names,
                    'true_category': idx_to_category.get(true_cat, f"Class_{true_cat}"),
                    'true_idx': true_cat,
                    'predicted_category': idx_to_category.get(pred_cat, f"Class_{pred_cat}"),
                    'predicted_idx': pred_cat,
                    'confidence': confidence,
                    'correct': true_cat == pred_cat
                }
                examples.append(example)
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "=" * 80)
    print("–ü–†–ò–ú–ï–†–´ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô")
    print("=" * 80)
    
    correct_count = 0
    for i, ex in enumerate(examples, 1):
        status = "‚úÖ" if ex['correct'] else "‚ùå"
        correct_count += 1 if ex['correct'] else 0
        
        print(f"\n{status} –ü—Ä–∏–º–µ—Ä {i}:")
        print(f"  –†–µ—Ü–µ–ø—Ç: {ex['recipe_name']}")
        print(f"  –ò—Å—Ç–∏–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {ex['true_category']}")
        print(f"  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {ex['predicted_category']} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {ex['confidence']:.2%})")
    
    accuracy = correct_count / len(examples)
    print(f"\nüìä –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö: {accuracy:.2%} ({correct_count}/{len(examples)})")
    
    return examples


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞."""
    parser = argparse.ArgumentParser(
        description='–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Terrazite AI',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python scripts/train_model.py                     # –û–±—É—á–µ–Ω–∏–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
  python scripts/train_model.py --epochs 100        # –£–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
  python scripts/train_model.py --batch-size 16     # –£–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
  python scripts/train_model.py --plot              # –°–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏
  python scripts/train_model.py --device cuda       # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU
        """
    )
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    parser.add_argument('--config', type=str, default=None,
                       help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)')
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    parser.add_argument('--batch-size', type=int, default=None,
                       help=f'–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {config.model.batch_size})')
    parser.add_argument('--learning-rate', type=float, default=None,
                       help=f'–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {config.model.learning_rate})')
    parser.add_argument('--epochs', type=int, default=None,
                       help=f'–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {config.model.epochs})')
    parser.add_argument('--weight-decay', type=float, default=None,
                       help=f'Weight decay (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {config.model.weight_decay})')
    parser.add_argument('--device', type=str, default='auto',
                       choices=['auto', 'cuda', 'cpu'],
                       help='–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (auto/cuda/cpu)')
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    parser.add_argument('--save-path', type=str, default=None,
                       help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)')
    parser.add_argument('--output-dir', type=str, default='checkpoints',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ü–∏–∏
    parser.add_argument('--plot', action='store_true',
                       help='–°–æ–∑–¥–∞–≤–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--test-samples', type=int, default=5,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –ø–æ–∫–∞–∑–∞')
    parser.add_argument('--quick-test', action='store_true',
                       help='–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç (2 —ç–ø–æ—Ö–∏, –º–∞–ª—ã–π –±–∞—Ç—á)')
    
    args = parser.parse_args()
    
    # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
    if args.quick_test:
        args.epochs = 2
        args.batch_size = 4
        args.plot = True
        logger.info("‚ö° –†–µ–∂–∏–º –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: epochs=2, batch_size=4")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    if args.device == 'auto':
        args.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"‚ÑπÔ∏è –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {args.device}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
    if args.device == 'cuda' and not torch.cuda.is_available():
        logger.warning("‚ö†Ô∏è CUDA –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞. –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ CPU.")
        args.device = 'cpu'
    
    try:
        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        trainer, history, metrics = train_model(args)
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        if args.test_samples > 0:
            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º test_loader –¥–ª—è –ø—Ä–∏–º–µ—Ä–æ–≤
            _, _, test_loader = trainer.prepare_dataloaders(
                batch_size=args.batch_size or config.model.batch_size,
                test_manifest='data/processed/data_manifest_test.csv'
            )
            test_predictions(trainer, test_loader, args.test_samples)
        
        # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        print("\n" + "=" * 80)
        print("üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        print("=" * 80)
        
        print("\nüìä –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò:")
        for key, value in metrics.items():
            if isinstance(value, (int, float)):
                if 'accuracy' in key:
                    print(f"  {key}: {value:.2%}")
                elif 'loss' in key:
                    print(f"  {key}: {value:.4f}")
                else:
                    print(f"  {key}: {value:.4f}")
        
        print(f"\nüìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {args.output_dir}")
        
        print("\nüîç –ß–¢–û –î–ê–õ–¨–®–ï:")
        print("  1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: python test_model_basic.py")
        print("  2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ API: uvicorn src.api.main:app --reload")
        print("  3. –û—Ç–∫—Ä–æ–π—Ç–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å: streamlit run streamlit_app.py")
        
    except FileNotFoundError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print("\nüí° –°–Ω–∞—á–∞–ª–∞ –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–∞–Ω–Ω—ã–µ:")
        print("   python scripts/create_data_manifest.py")
        print("   python scripts/prepare_image_dataset.py")
        sys.exit(1)
        
    except Exception as e:
        logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
