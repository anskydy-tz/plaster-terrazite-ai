"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π —Ç–µ—Ä—Ä–∞–∑–∏—Ç–æ–≤–æ–π —à—Ç—É–∫–∞—Ç—É—Ä–∫–∏.
"""
import tensorflow as tf
from tensorflow import keras
import numpy as np
from sklearn.model_selection import train_test_split
import logging
from typing import Tuple, Dict, List, Optional, Any
import json
import os
from pathlib import Path
import pandas as pd

from ..data.loader import DataLoader, RecipeLoader
from ..data.processor import DataProcessor

logger = logging.getLogger(__name__)


class ModelTrainer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.
    """
    
    def __init__(self, config_path: Optional[str] = None):
        self.config = self._load_config(config_path)
        self.data_processor = DataProcessor()
        self.model = None
        
    def _load_config(self, config_path: Optional[str]) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è."""
        default_config = {
            'input_shape': (224, 224, 3),
            'num_regression_outputs': 15,
            'num_classes': 5,
            'batch_size': 32,
            'epochs': 50,
            'learning_rate': 0.001,
            'validation_split': 0.2,
            'test_split': 0.1,
            'data_augmentation': True,
            'augmentation_params': {
                'rotation_range': 20,
                'width_shift_range': 0.2,
                'height_shift_range': 0.2,
                'horizontal_flip': True,
                'brightness_range': [0.8, 1.2]
            }
        }
        
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r') as f:
                user_config = json.load(f)
            default_config.update(user_config)
        
        return default_config
    
    def prepare_data(self, data_dir: str) -> Tuple:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
        
        Args:
            data_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏ (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ JSON —Å —Ä–µ—Ü–µ–ø—Ç–∞–º–∏)
        
        Returns:
            –ö–æ—Ä—Ç–µ–∂ —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {data_dir}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        images, recipes, aggregate_types = self.data_processor.load_dataset(data_dir)
        
        if len(images) == 0:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        y_regression = []
        y_classification = []
        
        for recipe in recipes:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Ä–µ—Ü–µ–ø—Ç–∞ (–ø—Ä–∏–≤–æ–¥–∏–º –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É 0-1)
            # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ä–µ—Ü–µ–ø—Ç –≤ –≤–µ–∫—Ç–æ—Ä —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã
            # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É
            recipe_vector = self._recipe_to_vector(recipe)
            y_regression.append(recipe_vector)
        
        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–æ–≤
        from sklearn.preprocessing import LabelEncoder, OneHotEncoder
        label_encoder = LabelEncoder()
        y_encoded = label_encoder.fit_transform(aggregate_types)
        
        # One-hot encoding –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        onehot_encoder = OneHotEncoder(sparse=False)
        y_onehot = onehot_encoder.fit_transform(y_encoded.reshape(-1, 1))
        
        y_classification = y_onehot
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        X_train, X_temp, y_reg_train, y_reg_temp, y_cls_train, y_cls_temp = train_test_split(
            images, y_regression, y_classification,
            test_size=self.config['validation_split'] + self.config['test_split'],
            random_state=42
        )
        
        # –î–µ–ª–∏–º temp –Ω–∞ validation –∏ test
        val_test_ratio = self.config['test_split'] / (self.config['validation_split'] + self.config['test_split'])
        
        X_val, X_test, y_reg_val, y_reg_test, y_cls_val, y_cls_test = train_test_split(
            X_temp, y_reg_temp, y_cls_temp,
            test_size=val_test_ratio,
            random_state=42
        )
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy arrays
        X_train = np.array(X_train)
        X_val = np.array(X_val)
        X_test = np.array(X_test)
        
        y_reg_train = np.array(y_reg_train)
        y_reg_val = np.array(y_reg_val)
        y_reg_test = np.array(y_reg_test)
        
        y_cls_train = np.array(y_cls_train)
        y_cls_val = np.array(y_cls_val)
        y_cls_test = np.array(y_cls_test)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        y_train = {
            'regression_output': y_reg_train,
            'classification_output': y_cls_train
        }
        
        y_val = {
            'regression_output': y_reg_val,
            'classification_output': y_cls_val
        }
        
        y_test = {
            'regression_output': y_reg_test,
            'classification_output': y_cls_test
        }
        
        logger.info(f"–î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")
        logger.info(f"  –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        logger.info(f"  –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_val)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        logger.info(f"  –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        
        return (X_train, y_train), (X_val, y_val), (X_test, y_test)
    
    def _recipe_to_vector(self, recipe: Dict) -> np.ndarray:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Ü–µ–ø—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã.
        
        Args:
            recipe: –°–ª–æ–≤–∞—Ä—å —Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ —Ä–µ—Ü–µ–ø—Ç–∞
        
        Returns:
            –í–µ–∫—Ç–æ—Ä –∑–Ω–∞—á–µ–Ω–∏–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ (–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö)
        """
        # –ó–∞–≥–ª—É—à–∫–∞: —Å–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ä–µ—Ü–µ–ø—Ç –≤ –≤–µ–∫—Ç–æ—Ä –∏–∑ self.config['num_regression_outputs'] –∑–Ω–∞—á–µ–Ω–∏–π
        vector = np.zeros(self.config['num_regression_outputs'])
        
        # –ü—Ä–∏–º–µ—Ä: –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 15 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏–∑ —Ä–µ—Ü–µ–ø—Ç–∞, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        # –í–∞–º –Ω—É–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ –ø–æ–¥ –≤–∞—à—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–µ—Ü–µ–ø—Ç–∞
        if 'binders' in recipe:
            # –°—É–º–º–∏—Ä—É–µ–º —Å–≤—è–∑—É—é—â–∏–µ
            binders_sum = sum(recipe['binders'].values())
            vector[0] = binders_sum / 1000.0  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –æ–±—â–∏–π –≤–µ—Å
        
        # ... –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è –¥—Ä—É–≥–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        
        return vector
    
    def build_model(self) -> Any:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏."""
        logger.info("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ü–∏–∫–ª–∏—á–µ—Å–∫–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞
        from .terrazite_model import TerraziteRecipeModel
        
        self.model = TerraziteRecipeModel(
            input_shape=self.config['input_shape'],
            num_regression_outputs=self.config['num_regression_outputs'],
            num_classes=self.config['num_classes'],
            learning_rate=self.config['learning_rate']
        )
        
        self.model.build_model()
        return self.model
    
    def train(self, train_data, val_data, callbacks=None):
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏.
        
        Args:
            train_data: –ö–æ—Ä—Ç–µ–∂ (X_train, y_train) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            val_data: –ö–æ—Ä—Ç–µ–∂ (X_val, y_val) –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            callbacks: –°–ø–∏—Å–æ–∫ callback'–æ–≤ Keras
        
        Returns:
            –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        """
        if self.model is None:
            self.build_model()
        
        logger.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...")
        
        history = self.model.train(
            train_data=train_data,
            val_data=val_data,
            epochs=self.config['epochs'],
            batch_size=self.config['batch_size'],
            callbacks=callbacks
        )
        
        return history
    
    def evaluate(self, test_data):
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            test_data: –ö–æ—Ä—Ç–µ–∂ (X_test, y_test) –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ train()")
        
        logger.info("–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        metrics = self.model.evaluate(test_data)
        
        return metrics
    
    def save_model(self, path: str = 'models/terrazite_model_final.h5'):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.
        """
        if self.model is None:
            raise ValueError("–ù–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        
        os.makedirs(os.path.dirname(path), exist_ok=True)
        self.model.save_model(path)
    
    def save_training_history(self, history, path: str = 'logs/training_history.json'):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è –≤ JSON.
        """
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç–∏–ø—ã Python
        history_serializable = {}
        for key, values in history.items():
            if isinstance(values, list):
                history_serializable[key] = [float(v) if isinstance(v, (np.floating, float)) else v for v in values]
            else:
                history_serializable[key] = float(values) if isinstance(values, (np.floating, float)) else values
        
        with open(path, 'w') as f:
            json.dump(history_serializable, f, indent=2)
        
        logger.info(f"–ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {path}")


def train_simple_classifier(data_dir: str, model_save_path: str = 'models/simple_classifier.joblib'):
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–æ—Å—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (Random Forest).
    """
    from .simple_classifier import SimpleAggregateClassifier
    from ..data.processor import DataProcessor
    
    logger.info("–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    processor = DataProcessor()
    images, _, aggregate_types = processor.load_dataset(data_dir)
    
    if len(images) == 0:
        raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    
    # –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    clf = SimpleAggregateClassifier()
    clf.fit(images, aggregate_types)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    clf.save(model_save_path)
    
    logger.info(f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {model_save_path}")
    return clf


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ModelTrainer")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∞
    num_samples = 100
    images = [np.random.rand(224, 224, 3).astype('float32') for _ in range(num_samples)]
    recipes = [{'binders': {'white_cement': 100, 'gray_cement': 50}} for _ in range(num_samples)]
    aggregate_types = np.random.choice(['–º—Ä–∞–º–æ—Ä', '–∫–≤–∞—Ä—Ü', '–≥—Ä–∞–Ω–∏—Ç', '—Å–ª—é–¥–∞', '–∏–∑–≤–µ—Å—Ç–Ω—è–∫'], size=num_samples)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    import tempfile
    
    with tempfile.TemporaryDirectory() as tmpdir:
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        images_dir = os.path.join(tmpdir, 'images')
        os.makedirs(images_dir, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–∑–∞–≥–ª—É—à–∫–∏)
        for i, img in enumerate(images):
            # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –Ω–æ –¥–ª—è —Ç–µ—Å—Ç–∞ –ø—Ä–æ—Å—Ç–æ —Å–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª
            with open(os.path.join(images_dir, f'img_{i}.npy'), 'wb') as f:
                np.save(f, img)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ—Ü–µ–ø—Ç—ã
        recipes_data = []
        for i, (recipe, agg_type) in enumerate(zip(recipes, aggregate_types)):
            recipe_data = {
                'sample_id': f'SAMPLE_{i}',
                'image_filename': f'img_{i}.npy',
                'recipe': recipe,
                'aggregate_type': agg_type
            }
            recipes_data.append(recipe_data)
        
        with open(os.path.join(tmpdir, 'recipes.json'), 'w') as f:
            json.dump(recipes_data, f, indent=2)
        
        print(f"–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã –≤ {tmpdir}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º ModelTrainer
        trainer = ModelTrainer()
        try:
            train_data, val_data, test_data = trainer.prepare_data(tmpdir)
            print(f"–î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã: train={len(train_data[0])}, val={len(val_data[0])}, test={len(test_data[0])}")
            
            model = trainer.build_model()
            print("–ú–æ–¥–µ–ª—å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞")
            
            # –î–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö, –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            print("–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ (–Ω—É–∂–Ω—ã —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)")
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞: {e}")
    
    print("\n‚úÖ ModelTrainer –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
